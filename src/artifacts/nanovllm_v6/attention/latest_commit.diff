commit 9a3a75b31db1d49d5ceeb25d648876ae917849cf
Author: Yang Yuxuan <yangyuxuanf1dt@gmail.com>
Date:   Thu Dec 11 02:28:46 2025 +0000

    Add mask_indptr conversion when passing 'packed_custom_mask' only for BatchPrefillWithPagedKVCacheWrapper.plan()

diff --git a/flashinfer/prefill.py b/flashinfer/prefill.py
index 41fac0e4..38bb5d0a 100755
--- a/flashinfer/prefill.py
+++ b/flashinfer/prefill.py
@@ -36,7 +36,7 @@ from .jit import (
 )
 from .cudnn import cudnn_batch_prefill_with_kv_cache
 from .page import get_seq_lens
-from .quantization import packbits, segment_packbits
+from .quantization import packbits, segment_packbits, _get_indptr_for_packed_mask
 from .utils import (
     log2e,
     FP4Tensor,
@@ -1768,6 +1768,9 @@ class BatchPrefillWithPagedKVCacheWrapper:
         self._batch_size = batch_size
         self._num_qo_heads = num_qo_heads
         self._num_kv_heads = num_kv_heads
+        
+        # TODO: should we check consistency between custom_mask and packed_custom_mask?
+        
         if custom_mask is not None or packed_custom_mask is not None:
             mask_indptr = _compute_page_mask_indptr(
                 qo_indptr,
@@ -1775,6 +1778,10 @@ class BatchPrefillWithPagedKVCacheWrapper:
                 paged_kv_last_page_len,
                 page_size,
             )
+        
+        if packed_custom_mask is not None and custom_mask is None:
+            mask_indptr = _get_indptr_for_packed_mask(mask_indptr)
+        
         if packed_custom_mask is None and custom_mask is not None:
             # create packed custom mask from custom mask
             packed_custom_mask, mask_indptr = segment_packbits(
diff --git a/flashinfer/quantization.py b/flashinfer/quantization.py
index 4e279ab5..f9e0cbe1 100644
--- a/flashinfer/quantization.py
+++ b/flashinfer/quantization.py
@@ -77,6 +77,14 @@ def packbits(x: torch.Tensor, bitorder: str = "big") -> torch.Tensor:
     """
     return _packbits(x, bitorder)
 
+def _get_indptr_for_packed_mask(
+    indptr: torch.Tensor,
+):
+    seglen = indptr[1:] - indptr[:-1]
+    packed_len = (seglen + 7) // 8
+    indptr_new = torch.zeros(len(indptr), dtype=indptr.dtype, device=indptr.device)
+    indptr_new[1:] = torch.cumsum(packed_len, 0)
+    return indptr_new
 
 @flashinfer_api
 def segment_packbits(
@@ -125,10 +133,7 @@ def segment_packbits(
     --------
     packbits
     """
-    seglen = indptr[1:] - indptr[:-1]
-    packed_len = (seglen + 7) // 8
-    indptr_new = torch.zeros(len(indptr), dtype=indptr.dtype, device=indptr.device)
-    indptr_new[1:] = torch.cumsum(packed_len, 0)
+    indptr_new = _get_indptr_for_packed_mask(indptr)
     output_nnzs = indptr_new[-1].item()
 
     device = x.device
